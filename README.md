# Shallow Neural Network from Scratch - Handwritten Digit Classification

This project implements a **shallow neural network** from scratch using **NumPy** to classify handwritten digits. The model is trained using **forward and backward propagation**, **ReLU activation**, **Softmax output**, and **gradient descent** for optimization.

## ðŸš€ Features
- Fully-connected two-layer neural network  
- ReLU activation function and Softmax for multi-class classification  
- One-hot encoding for labels  
- Gradient descent optimization with parameter updates  
- Accuracy evaluation and visualization of predictions  

## ðŸ›  Technologies Used
- Python  
- NumPy  
- Matplotlib  

## ðŸ“Œ How It Works
1. **Initialize Weights**: Randomly initializes network weights and biases.  
2. **Forward Propagation**: Computes activations using ReLU and Softmax.  
3. **Backward Propagation**: Computes gradients and updates parameters.  
4. **Training**: Runs gradient descent for optimization.  
5. **Prediction & Visualization**: Tests the model on validation data and displays results.  

## ðŸ”§ Installation
1. Clone the repository:
   ```bash
   git clone https://github.com/KarimBassel/Shallow-Neural-Network.git
